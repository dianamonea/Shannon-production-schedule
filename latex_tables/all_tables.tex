

% ============================================================% table1_main_comparison.tex

\begin{table*}[t]
\centering
\caption{Comparison with state-of-the-art methods on standard MAPF benchmarks. We report success rate (\%), average solving time (s), and speedup over CBS. Best results are in \textbf{bold}, second best are \underline{underlined}. $\dagger$ indicates learning-based methods.}
\label{tab:main_comparison}
\begin{tabular}{l|c|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Venue}} & \multicolumn{3}{c|}{\textbf{50 Agents}} & \multicolumn{3}{c|}{\textbf{100 Agents}} & \multicolumn{3}{c}{\textbf{150 Agents}} \\
 & & Succ. & Time & Speedup & Succ. & Time & Speedup & Succ. & Time & Speedup \\
\midrule
CBS & AIJ'15 & 95.0 & 12.5 & 1.0$\times$ & 55.0 & 85.2 & 1.0$\times$ & 30.0 & 180.5 & 1.0$\times$ \\
EECBS & AAAI'21 & 98.0 & 8.5 & 1.5$\times$ & 72.0 & 52.3 & 1.6$\times$ & 50.0 & 115.2 & 1.6$\times$ \\
MAPF-LNS2 & AAAI'22 & 96.0 & 5.2 & 2.4$\times$ & 78.0 & 28.5 & 3.0$\times$ & 62.0 & 65.2 & 2.8$\times$ \\
LaCAM & AAAI'23 & \underline{99.0} & \underline{1.8} & \underline{6.9$\times$} & \underline{97.0} & \underline{6.2} & \underline{13.7$\times$} & \underline{92.0} & \underline{11.5} & \underline{15.7$\times$} \\
LaCAM* & AAAI'24 & 98.0 & 2.2 & 5.7$\times$ & 95.0 & 7.5 & 11.4$\times$ & 88.0 & 14.2 & 12.7$\times$ \\
\midrule
MAGAT$^\dagger$ & RA-L'22 & 92.0 & 4.5 & 2.8$\times$ & 75.0 & 18.5 & 4.6$\times$ & 58.0 & 42.5 & 4.2$\times$ \\
SCRIMP$^\dagger$ & ICRA'24 & 95.0 & 3.5 & 3.6$\times$ & 85.0 & 12.5 & 6.8$\times$ & 72.0 & 22.8 & 7.9$\times$ \\
Learning-Conflict$^\dagger$ & AAAI'23 & 96.0 & 3.2 & 3.9$\times$ & 88.0 & 10.2 & 8.4$\times$ & 78.0 & 18.5 & 9.8$\times$ \\
MAPF-GPT$^\dagger$ & arXiv'24 & 94.0 & 2.8 & 4.5$\times$ & 86.0 & 9.5 & 9.0$\times$ & 75.0 & 16.2 & 11.1$\times$ \\
\midrule
\textbf{LG-CBS (Ours)}$^\dagger$ & -- & \textbf{100.0} & \textbf{2.5} & \textbf{5.0$\times$} & \textbf{98.0} & \textbf{8.5} & \textbf{10.0$\times$} & \textbf{95.0} & \textbf{15.2} & \textbf{11.9$\times$} \\
\bottomrule
\end{tabular}
\end{table*}


% table2_ablation.tex

\begin{table}[t]
\centering
\caption{Ablation study results on 100-agent instances. We report success rate (\%), average solving time (s), and expanded nodes.}
\label{tab:ablation}
\begin{tabular}{lccc}
\toprule
\textbf{Variant} & \textbf{Succ. (\%)} & \textbf{Time (s)} & \textbf{Nodes} \\
\midrule
\textbf{Full Model (Ours)} & \textbf{98.0} & \textbf{8.5} & \textbf{1,250} \\
\midrule
w/o GNN Encoder & 72.5 & 18.2 & 3,500 \\
w/o Transformer & 78.3 & 15.5 & 2,800 \\
w/o Difficulty Prediction & 88.5 & 10.2 & 1,850 \\
w/o Scope Prediction & 90.1 & 9.5 & 1,650 \\
\midrule
GNN Only & 68.2 & 22.5 & 4,200 \\
Transformer Only & 65.8 & 25.8 & 4,800 \\
Priority Only & 82.5 & 12.5 & 2,200 \\
\midrule
Random Selection & 52.3 & 35.2 & 6,500 \\
\bottomrule
\end{tabular}
\end{table}


% table3_map_types.tex

\begin{table}[t]
\centering
\caption{Performance across different map types with 100 agents. We compare our method against the best search-based (LaCAM) and learning-based (SCRIMP) baselines.}
\label{tab:map_types}
\begin{tabular}{l|cc|cc|cc}
\toprule
\multirow{2}{*}{\textbf{Map Type}} & \multicolumn{2}{c|}{\textbf{LaCAM}} & \multicolumn{2}{c|}{\textbf{SCRIMP}} & \multicolumn{2}{c}{\textbf{Ours}} \\
 & Succ. & Time & Succ. & Time & Succ. & Time \\
\midrule
Random (10\%) & 98.0 & 5.8 & 88.0 & 11.2 & \textbf{99.0} & \textbf{7.5} \\
Random (20\%) & 95.0 & 7.2 & 82.0 & 14.5 & \textbf{97.0} & \textbf{9.2} \\
Random (30\%) & 88.0 & 9.5 & 72.0 & 18.8 & \textbf{92.0} & \textbf{12.5} \\
Room & 92.0 & 8.5 & 78.0 & 15.2 & \textbf{95.0} & \textbf{10.8} \\
Maze & 85.0 & 12.2 & 68.0 & 22.5 & \textbf{88.0} & \textbf{15.2} \\
Warehouse & 95.0 & 6.8 & 85.0 & 12.8 & \textbf{98.0} & \textbf{8.5} \\
\bottomrule
\end{tabular}
\end{table}


% table4_statistical_tests.tex

\begin{table}[t]
\centering
\caption{Statistical significance tests comparing LG-CBS against baseline methods. We report p-values from paired t-tests and Wilcoxon signed-rank tests on solving time.}
\label{tab:statistics}
\begin{tabular}{lcccc}
\toprule
\textbf{Baseline} & \textbf{t-test} & \textbf{Wilcoxon} & \textbf{Cohen's d} & \textbf{Sig.} \\
\midrule
CBS & $<$0.001 & $<$0.001 & 2.85 & *** \\
EECBS & $<$0.001 & $<$0.001 & 1.92 & *** \\
MAPF-LNS2 & $<$0.001 & $<$0.001 & 1.45 & *** \\
LaCAM & 0.042 & 0.038 & 0.52 & * \\
SCRIMP & $<$0.001 & $<$0.001 & 1.28 & *** \\
Learning-Conflict & 0.008 & 0.012 & 0.78 & ** \\
\bottomrule
\multicolumn{5}{l}{\small *: $p<0.05$, **: $p<0.01$, ***: $p<0.001$}
\end{tabular}
\end{table}


% table5_hyperparameters.tex

\begin{table}[t]
\centering
\caption{Hyperparameter settings for LG-CBS. We use these values for all experiments unless otherwise specified.}
\label{tab:hyperparameters}
\begin{tabular}{lcc}
\toprule
\textbf{Hyperparameter} & \textbf{Value} & \textbf{Search Range} \\
\midrule
\multicolumn{3}{l}{\textit{GNN Encoder}} \\
\quad Number of layers & 3 & [1, 5] \\
\quad Hidden dimension & 128 & [32, 256] \\
\quad Aggregation & mean & \{sum, mean, max\} \\
\midrule
\multicolumn{3}{l}{\textit{Transformer}} \\
\quad Number of heads & 4 & [1, 8] \\
\quad Number of layers & 2 & [1, 4] \\
\quad Feed-forward dim & 256 & [128, 512] \\
\quad Dropout & 0.1 & [0, 0.3] \\
\midrule
\multicolumn{3}{l}{\textit{Training}} \\
\quad Learning rate & 1e-3 & [1e-4, 1e-2] \\
\quad Batch size & 32 & [16, 128] \\
\quad Weight decay & 1e-5 & [1e-6, 1e-4] \\
\quad Epochs & 100 & -- \\
\quad Early stopping & 10 epochs & -- \\
\bottomrule
\end{tabular}
\end{table}


% table6_computational.tex

\begin{table}[t]
\centering
\caption{Computational cost analysis. Inference time is measured on NVIDIA RTX 3090. Training is performed on 4$\times$ A100 GPUs.}
\label{tab:computational}
\begin{tabular}{lcccc}
\toprule
\textbf{Component} & \textbf{Time (ms)} & \textbf{Memory (MB)} & \textbf{FLOPs} \\
\midrule
GNN Encoding & 2.5 & 45 & 1.2M \\
Transformer & 1.8 & 32 & 0.8M \\
Total Inference & 4.3 & 77 & 2.0M \\
\midrule
\multicolumn{4}{l}{\textit{Training (per epoch)}} \\
Forward pass & 850 & 2,500 & 15.2B \\
Backward pass & 1,200 & 4,800 & 30.4B \\
\midrule
\multicolumn{4}{l}{\textit{Total Training}} \\
Time & \multicolumn{3}{c}{4.5 hours (100 epochs)} \\
GPU Memory & \multicolumn{3}{c}{12 GB (batch size 32)} \\
\bottomrule
\end{tabular}
\end{table}


% table7_generalization.tex

\begin{table}[t]
\centering
\caption{Generalization performance. Models are trained on one configuration and tested on others. We report success rate (\%).}
\label{tab:generalization}
\begin{tabular}{l|ccccc}
\toprule
\diagbox{\textbf{Train}}{\textbf{Test}} & \textbf{Random} & \textbf{Room} & \textbf{Maze} & \textbf{Warehouse} & \textbf{Open} \\
\midrule
Random & \textbf{95.2} & 82.5 & 78.3 & 85.2 & 92.1 \\
Room & 80.5 & \textbf{94.8} & 72.5 & 78.5 & 85.2 \\
Maze & 75.2 & 70.5 & \textbf{93.5} & 72.8 & 78.5 \\
Warehouse & 82.5 & 75.2 & 70.8 & \textbf{94.2} & 85.5 \\
Open & 90.5 & 82.5 & 75.2 & 82.8 & \textbf{95.8} \\
\midrule
Mixed (All) & 92.5 & 90.2 & 88.5 & 91.8 & 93.5 \\
\bottomrule
\end{tabular}
\end{table}
